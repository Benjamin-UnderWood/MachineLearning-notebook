{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    "listOfTokens = re.split(r'\\W+', mySent)\n",
    "print(listOfTokens)\n",
    "# tokenList = [tok for tok in listOfTokens if len(tok) > 0]\n",
    "# tokenList = [tok.lower() for tok in listOfTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param dataSet: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocabSet = set([])\n",
    "    # create empty set\n",
    "    for document in dataSet:\n",
    "        # 操作符 | 用于求两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "        # union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocabList: 所有单词集合列表\n",
    "    :param inputSet: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    returnVec = [0] * len(vocabList) # [0,0......]\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print (\"the word: %s is not in my Vocabulary!\" %word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据优化版本\n",
    "    :param trainMatrix: 文件单词矩阵\n",
    "    :param trainCategory: 文件对应的类别\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 总文件数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 总单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词出现次数列表\n",
    "    # p0Num 正常的统计\n",
    "    # p1Num 侮辱的统计\n",
    "    p0Num = ones(numWords)#[0,0......]->[1,1,1,1,1.....]\n",
    "    p1Num = ones(numWords)\n",
    "    # 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）\n",
    "    # p0Denom 正常的统计\n",
    "    # p1Denom 侮辱的统计\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            # 累加辱骂词的频次\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对每篇文章的辱骂的频次 进行统计汇总\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "            # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "            p1Vect = log(p1Num / p1Denom)\n",
    "            # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "            p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "    # 将乘法转换为加法\n",
    "    乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "    加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param pClass1: 类别1，侮辱性文件的出现概率    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 大家可能会发现，上面的计算公式，没有除以贝叶斯准则的公式的分母，也就是 P(w) （P(w) 指的是此文档在所有的文档中出现的概率）就进行概率大小的比较了，\n",
    "    # 因为 P(w) 针对的是包含侮辱和非侮辱的全部文档，所以 P(w) 是相同的。\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1) # P(w|c1) * P(c1) ，即贝叶斯准则的分子\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) # P(w|c0) * P(c0) ，即贝叶斯准则的分子·\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分文本\n",
    "def textParse(bigString):\n",
    "    '''\n",
    "    Desc:        接收一个大字符串并将其解析为字符串列表\n",
    "    Args:        bigString -- 大字符串\n",
    "    Returns:        去掉少于 2 个字符的字符串，并将所有字符串转换为小写，返回字符串列表\n",
    "    '''\n",
    "    import re\n",
    "    # 使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串\n",
    "    listOfTokens = re.split(r'\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "\n",
    "def spamTest():\n",
    "    '''\n",
    "    Desc:        对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    Args:        none\n",
    "    Returns:        对测试集中的每封邮件进行分类，若邮件分类错误，则错误数加 1，最后返回总的错误百分比。\n",
    "    '''\n",
    "    docList = []\n",
    "    classList = []\n",
    "    for i in range(1, 26):\n",
    "        # 切分，解析数据，并归类为 1 类别 \n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, 'rb').read().decode('utf8','ignore'))\n",
    "        docList.append(wordList)\n",
    "        classList.append(1)\n",
    "        # 切分，解析数据，并归类为 0 类别 垃圾邮件\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, 'rb').read().decode('utf8','ignore'))\n",
    "        docList.append(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    # 创建词汇表\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    \n",
    "    # 随机取 10 个邮件用来测试\n",
    "    for _ in range(10):\n",
    "        # random.uniform(x, y) 随机生成一个范围为 x - y 的实数\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "        \n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n",
    "\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    \n",
    "    print('the errorCount is: ', errorCount)\n",
    "    print('the testSet length is :', len(testSet))\n",
    "    print('the error rate is :', float(errorCount)/len(testSet))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the errorCount is:  0\n",
      "the testSet length is : 10\n",
      "the error rate is : 0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open('email/spam/17.txt', 'rb').read().decode('utf8','ignore')\n",
    "# textParse(open('email/ham/6.txt', 'rb').read().decode('utf8','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-986bf18bcfd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabList' is not defined"
     ]
    }
   ],
   "source": [
    "vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "docList = []\n",
    "classList = []\n",
    "for i in range(1, 26):\n",
    "    # 切分，解析数据，并归类为 1 类别 \n",
    "    wordList = textParse(open('email/spam/%d.txt' % i, 'rb').read().decode('utf8','ignore'))\n",
    "    docList.append(wordList)\n",
    "    classList.append(1)\n",
    "    \n",
    "    # 切分，解析数据，并归类为 0 类别 垃圾邮件\n",
    "    wordList = textParse(open('email/ham/%d.txt' % i, 'rb').read().decode('utf8','ignore'))\n",
    "    docList.append(wordList)\n",
    "    classList.append(0)\n",
    "    \n",
    "# 创建词汇表\n",
    "vocabList = createVocabList(docList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['changes',\n",
       " 'answer',\n",
       " 'finance',\n",
       " 'gain',\n",
       " 'bathroom',\n",
       " 'invitation',\n",
       " 'wilson',\n",
       " 'location',\n",
       " 'museum',\n",
       " 'insights',\n",
       " '513',\n",
       " 'borders',\n",
       " 'note',\n",
       " 'office',\n",
       " 'storage',\n",
       " 'logged',\n",
       " 'hours',\n",
       " '120',\n",
       " 'pretty',\n",
       " 'with',\n",
       " 'out',\n",
       " 'cheap',\n",
       " 'items',\n",
       " 'questions',\n",
       " 'carlo',\n",
       " 'automatically',\n",
       " 'ems',\n",
       " 'mail',\n",
       " '25mg',\n",
       " 'fans',\n",
       " 'your',\n",
       " 'top',\n",
       " 'giants',\n",
       " 'fractal',\n",
       " 'worldwide',\n",
       " 'going',\n",
       " 'cannot',\n",
       " 'source',\n",
       " 'expo',\n",
       " '588',\n",
       " 'thread',\n",
       " 'two',\n",
       " 'only',\n",
       " 'computer',\n",
       " 'pages',\n",
       " 'sky',\n",
       " 'died',\n",
       " 'holiday',\n",
       " 'done',\n",
       " 'dusty',\n",
       " 'experts',\n",
       " '66343',\n",
       " 'specifically',\n",
       " 'assistance',\n",
       " 'endorsed',\n",
       " 'ofejacu1ate',\n",
       " 'owner',\n",
       " 'brandviagra',\n",
       " 'regards',\n",
       " 'window',\n",
       " 'meet',\n",
       " 'treat',\n",
       " 'upload',\n",
       " 'thank',\n",
       " 'cats',\n",
       " 'jquery',\n",
       " 'another',\n",
       " 'brands',\n",
       " 'who',\n",
       " 'starting',\n",
       " 'hermes',\n",
       " 'find',\n",
       " 'pick',\n",
       " 'support',\n",
       " 'butt',\n",
       " 'express',\n",
       " 'naturalpenisenhancement',\n",
       " 'call',\n",
       " 'files',\n",
       " '50092',\n",
       " 'through',\n",
       " 'past',\n",
       " '292',\n",
       " '396',\n",
       " 'free',\n",
       " 'julius',\n",
       " 'mathematics',\n",
       " 'inches',\n",
       " 'now',\n",
       " 'writing',\n",
       " 'chapter',\n",
       " 'oris',\n",
       " 'good',\n",
       " 'hydrocodone',\n",
       " 'thirumalai',\n",
       " 'low',\n",
       " 'said',\n",
       " 'number',\n",
       " 'party',\n",
       " 'bike',\n",
       " 'interesting',\n",
       " 'scifinance',\n",
       " 'announcement',\n",
       " 'doctor',\n",
       " 'aged',\n",
       " 'way',\n",
       " 'approach',\n",
       " '750',\n",
       " 'could',\n",
       " 'view',\n",
       " 'commented',\n",
       " 'effective',\n",
       " 'turd',\n",
       " 'advocate',\n",
       " 'quantitative',\n",
       " 'professional',\n",
       " 'art',\n",
       " 'biggerpenis',\n",
       " 'dhl',\n",
       " 'stuff',\n",
       " 'rock',\n",
       " 'the',\n",
       " 'issues',\n",
       " 'color',\n",
       " 'millions',\n",
       " 'trusted',\n",
       " 'download',\n",
       " 'germany',\n",
       " 'team',\n",
       " 'sliding',\n",
       " 'codeine',\n",
       " 'focus',\n",
       " 'featured',\n",
       " 'benoit',\n",
       " 'don',\n",
       " 'monte',\n",
       " 'pls',\n",
       " 'approved',\n",
       " 'prepared',\n",
       " 'can',\n",
       " 'analgesic',\n",
       " 'harderecetions',\n",
       " 'fine',\n",
       " 'rent',\n",
       " 'troy',\n",
       " 'night',\n",
       " 'over',\n",
       " 'bags',\n",
       " 'dior',\n",
       " '100m',\n",
       " 'yay',\n",
       " 'bettererections',\n",
       " 'vivek',\n",
       " 'per',\n",
       " 'leaves',\n",
       " 'using',\n",
       " 'spaying',\n",
       " 'decision',\n",
       " 'income',\n",
       " 'courier',\n",
       " 'held',\n",
       " 'since',\n",
       " '5mg',\n",
       " '199',\n",
       " '325',\n",
       " '366',\n",
       " 'chance',\n",
       " 'fedex',\n",
       " 'online',\n",
       " 'ambiem',\n",
       " 'mandarin',\n",
       " 'opioid',\n",
       " 'would',\n",
       " 'saw',\n",
       " 'fda',\n",
       " '2007',\n",
       " 'jpgs',\n",
       " 'peter',\n",
       " 'nvidia',\n",
       " 'generates',\n",
       " 'methylmorphine',\n",
       " 'model',\n",
       " 'cuda',\n",
       " 'not',\n",
       " 'lists',\n",
       " 'groups',\n",
       " 'must',\n",
       " 'storedetailview_98',\n",
       " 'increase',\n",
       " 'betterejacu1ation',\n",
       " 'running',\n",
       " 'ones',\n",
       " 'update',\n",
       " 'doors',\n",
       " 'cards',\n",
       " 'his',\n",
       " 'buyviagra',\n",
       " 'femaleviagra',\n",
       " 'while',\n",
       " '86152',\n",
       " 'thickness',\n",
       " 'phone',\n",
       " 'father',\n",
       " 'gpu',\n",
       " 'class',\n",
       " 'door',\n",
       " 'notification',\n",
       " 'noprescription',\n",
       " 'watchesstore',\n",
       " 'pro',\n",
       " 'narcotic',\n",
       " 'tabs',\n",
       " 'heard',\n",
       " 'dont',\n",
       " 'was',\n",
       " 'all',\n",
       " 'reputable',\n",
       " 'cat',\n",
       " 'moneyback',\n",
       " 'much',\n",
       " 'back',\n",
       " 'capabilities',\n",
       " 'talked',\n",
       " 'store',\n",
       " 'have',\n",
       " 'whybrew',\n",
       " 'doggy',\n",
       " 'such',\n",
       " 'just',\n",
       " 'new',\n",
       " 'got',\n",
       " 'concise',\n",
       " 'file',\n",
       " 'care',\n",
       " 'eugene',\n",
       " 'address',\n",
       " 'docs',\n",
       " 'arvind',\n",
       " 'working',\n",
       " 'design',\n",
       " 'methods',\n",
       " 'significantly',\n",
       " 'status',\n",
       " 'life',\n",
       " 'genuine',\n",
       " 'stepp',\n",
       " 'code',\n",
       " 'required',\n",
       " 'softwares',\n",
       " 'cost',\n",
       " 'strategy',\n",
       " 'latest',\n",
       " 'placed',\n",
       " 'attaching',\n",
       " 'enough',\n",
       " 'cca',\n",
       " 'that',\n",
       " 'chinese',\n",
       " 'mom',\n",
       " 'welcome',\n",
       " 'mandelbrot',\n",
       " 'gucci',\n",
       " 'cs5',\n",
       " 'guaranteeed',\n",
       " 'cartier',\n",
       " 'runs',\n",
       " 'discreet',\n",
       " 'ready',\n",
       " 'having',\n",
       " 'used',\n",
       " 'off',\n",
       " 'hangzhou',\n",
       " 'youre',\n",
       " '322',\n",
       " 'customized',\n",
       " 'well',\n",
       " 'those',\n",
       " 'fermi',\n",
       " '562',\n",
       " 'visa',\n",
       " 'titles',\n",
       " 'perhaps',\n",
       " 'page',\n",
       " 'differ',\n",
       " 'articles',\n",
       " 'exhibit',\n",
       " 'incoming',\n",
       " 'louis',\n",
       " 'email',\n",
       " '138',\n",
       " 'series',\n",
       " 'hope',\n",
       " 'winter',\n",
       " 'both',\n",
       " 'financial',\n",
       " 'cheers',\n",
       " 'time',\n",
       " 'ryan',\n",
       " 'plus',\n",
       " '180',\n",
       " 'girl',\n",
       " 'google',\n",
       " 'below',\n",
       " 'programming',\n",
       " 'told',\n",
       " 'hotels',\n",
       " 'once',\n",
       " 'explosive',\n",
       " 'speedpost',\n",
       " 'year',\n",
       " 'had',\n",
       " '130',\n",
       " 'shipment',\n",
       " 'forward',\n",
       " 'things',\n",
       " 'thought',\n",
       " 'each',\n",
       " 'core',\n",
       " 'located',\n",
       " 'members',\n",
       " 'yesterday',\n",
       " 'focusing',\n",
       " 'service',\n",
       " 'name',\n",
       " 'tent',\n",
       " 'viagranoprescription',\n",
       " '174623',\n",
       " 'let',\n",
       " 'pharmacy',\n",
       " 'length',\n",
       " 'enjoy',\n",
       " 'days',\n",
       " 'creation',\n",
       " 'october',\n",
       " 'experience',\n",
       " 'today',\n",
       " '625',\n",
       " 'how',\n",
       " 'job',\n",
       " 'www',\n",
       " 'private',\n",
       " 'faster',\n",
       " 'but',\n",
       " 'knew',\n",
       " 'style',\n",
       " 'of_penisen1argement',\n",
       " 'requested',\n",
       " '225',\n",
       " 'train',\n",
       " 'transformed',\n",
       " 'right',\n",
       " 'products',\n",
       " 'watches',\n",
       " 'ideas',\n",
       " 'lunch',\n",
       " 'shipping',\n",
       " 'grounds',\n",
       " 'possible',\n",
       " 'important',\n",
       " 'listed',\n",
       " 'pricing',\n",
       " 'try',\n",
       " 'gas',\n",
       " 'reservation',\n",
       " 'via',\n",
       " 'quality',\n",
       " 'ultimate',\n",
       " 'most',\n",
       " 'behind',\n",
       " 'extended',\n",
       " 'use',\n",
       " 'incredib1e',\n",
       " 'reliever',\n",
       " 'nature',\n",
       " 'china',\n",
       " 'great',\n",
       " 'being',\n",
       " 'assigning',\n",
       " 'expertise',\n",
       " 'what',\n",
       " 'wasn',\n",
       " 'changing',\n",
       " 'dozen',\n",
       " 'longer',\n",
       " 'competitive',\n",
       " 'favorite',\n",
       " 'success',\n",
       " 'learn',\n",
       " 'brained',\n",
       " 'jewerly',\n",
       " 'work',\n",
       " 'more',\n",
       " 'zolpidem',\n",
       " 'when',\n",
       " 'risk',\n",
       " 'least',\n",
       " '14th',\n",
       " 'drugs',\n",
       " 'specifications',\n",
       " 'accepted',\n",
       " 'went',\n",
       " 'recieve',\n",
       " 'site',\n",
       " 'bad',\n",
       " 'sure',\n",
       " 'one',\n",
       " '300x',\n",
       " 'prices',\n",
       " 'than',\n",
       " 'features',\n",
       " 'photoshop',\n",
       " 'coast',\n",
       " 'ordercializviagra',\n",
       " 'any',\n",
       " 'strategic',\n",
       " 'also',\n",
       " 'follow',\n",
       " '0nline',\n",
       " 'hamm',\n",
       " 'copy',\n",
       " 'tour',\n",
       " 'game',\n",
       " 'full',\n",
       " 'sounds',\n",
       " 'ferguson',\n",
       " 'sorry',\n",
       " 'came',\n",
       " 'doing',\n",
       " 'oem',\n",
       " 'phentermin',\n",
       " 'thousand',\n",
       " 'instead',\n",
       " 'think',\n",
       " 'example',\n",
       " 'huge',\n",
       " 'major',\n",
       " 'program',\n",
       " 'delivery',\n",
       " 'car',\n",
       " 'moderate',\n",
       " 'sophisticated',\n",
       " 'buy',\n",
       " 'station',\n",
       " 'pill',\n",
       " 'blue',\n",
       " 'mailing',\n",
       " 'thanks',\n",
       " 'school',\n",
       " 'gains',\n",
       " '203',\n",
       " 'town',\n",
       " 'jay',\n",
       " 'enabled',\n",
       " '291',\n",
       " 'order',\n",
       " 'bargains',\n",
       " 'place',\n",
       " 'share',\n",
       " '1924',\n",
       " 'parallel',\n",
       " 'about',\n",
       " 'derivatives',\n",
       " 'grow',\n",
       " 'where',\n",
       " '50mg',\n",
       " 'adobe',\n",
       " 'freeviagra',\n",
       " 'definitely',\n",
       " 'does',\n",
       " 'day',\n",
       " 'suggest',\n",
       " 'arolexbvlgari',\n",
       " 'tickets',\n",
       " 'pills',\n",
       " '2010',\n",
       " 'com',\n",
       " 'canadian',\n",
       " 'superb',\n",
       " 'natural',\n",
       " 'ups',\n",
       " 'add',\n",
       " 'bin',\n",
       " 'from',\n",
       " 'acrobat',\n",
       " 'reply',\n",
       " 'tokyo',\n",
       " 'away',\n",
       " 'inspired',\n",
       " 'jar',\n",
       " 'supporting',\n",
       " 'windows',\n",
       " 'warranty',\n",
       " '15mg',\n",
       " 'wallets',\n",
       " 'should',\n",
       " 'python',\n",
       " 'roofer',\n",
       " 'plugin',\n",
       " 'mathematician',\n",
       " 'for',\n",
       " '385',\n",
       " 'pavilion',\n",
       " 'mandatory',\n",
       " 'launch',\n",
       " 'book',\n",
       " 'opportunity',\n",
       " 'might',\n",
       " 'guy',\n",
       " 'earn',\n",
       " 'discount',\n",
       " 'vuitton',\n",
       " 'everything',\n",
       " 'needed',\n",
       " 'thing',\n",
       " 'survive',\n",
       " 'keep',\n",
       " 'intenseorgasns',\n",
       " 'amex',\n",
       " 'close',\n",
       " 'please',\n",
       " 'these',\n",
       " 'retirement',\n",
       " 'create',\n",
       " 'vicodin',\n",
       " 'safe',\n",
       " 'accept',\n",
       " 'often',\n",
       " 'website',\n",
       " '200',\n",
       " 'riding',\n",
       " 'fundamental',\n",
       " 'they',\n",
       " 'jocelyn',\n",
       " 'access',\n",
       " 'pain',\n",
       " 'some',\n",
       " 'plane',\n",
       " 'computing',\n",
       " 'connection',\n",
       " 'far',\n",
       " 'mba',\n",
       " 'couple',\n",
       " 'brand',\n",
       " '30mg',\n",
       " 'horn',\n",
       " 'certified',\n",
       " 'others',\n",
       " 'fast',\n",
       " 'ma1eenhancement',\n",
       " 'credit',\n",
       " 'cold',\n",
       " 'home',\n",
       " 'kerry',\n",
       " 'amazing',\n",
       " 'like',\n",
       " 'wrote',\n",
       " 'john',\n",
       " 'high',\n",
       " 'been',\n",
       " 'works',\n",
       " 'message',\n",
       " 'and',\n",
       " 'rain',\n",
       " 'hello',\n",
       " 'magazine',\n",
       " 'item',\n",
       " 'are',\n",
       " 'price',\n",
       " 'hotel',\n",
       " 'discussions',\n",
       " 'there',\n",
       " 'creative',\n",
       " 'this',\n",
       " 'signed',\n",
       " 'glimpse',\n",
       " 'comment',\n",
       " 'same',\n",
       " 'information',\n",
       " 'sent',\n",
       " 'zach',\n",
       " 'web',\n",
       " 'http',\n",
       " 'wholesale',\n",
       " 'hommies',\n",
       " '570',\n",
       " 'knocking',\n",
       " 'network',\n",
       " 'pictures',\n",
       " 'based',\n",
       " 'then',\n",
       " '195',\n",
       " 'save',\n",
       " 'inside',\n",
       " 'has',\n",
       " 'inform',\n",
       " 'encourage',\n",
       " 'herbal',\n",
       " 'see',\n",
       " 'designed',\n",
       " 'severepain',\n",
       " 'generation',\n",
       " 'february',\n",
       " 'management',\n",
       " 'will',\n",
       " 'watson',\n",
       " 'them',\n",
       " 'business',\n",
       " 'modelling',\n",
       " 'yourpenis',\n",
       " 'wilmott',\n",
       " 'release',\n",
       " 'uses',\n",
       " 'hold',\n",
       " 'tiffany',\n",
       " 'trip',\n",
       " 'percocet',\n",
       " 'received',\n",
       " 'want',\n",
       " 'wednesday',\n",
       " 'because',\n",
       " 'level',\n",
       " '492',\n",
       " 'tool',\n",
       " 'thailand',\n",
       " 'jose',\n",
       " 'inconvenience',\n",
       " '219',\n",
       " 'made',\n",
       " 'sites',\n",
       " 'know',\n",
       " 'get',\n",
       " 'edit',\n",
       " 'help',\n",
       " 'looking',\n",
       " 'too',\n",
       " 'famous',\n",
       " '156',\n",
       " 'tesla',\n",
       " 'supplement',\n",
       " 'link',\n",
       " '129',\n",
       " 'control',\n",
       " 'serial',\n",
       " 'contact',\n",
       " 'lined',\n",
       " 'includes',\n",
       " '119',\n",
       " '100',\n",
       " 'functionalities',\n",
       " 'need',\n",
       " 'withoutprescription',\n",
       " 'scenic',\n",
       " '10mg',\n",
       " 'check',\n",
       " 'drunk',\n",
       " 'take',\n",
       " 'linkedin',\n",
       " 'safest',\n",
       " 'improving',\n",
       " 'easily',\n",
       " 'development',\n",
       " 'prototype',\n",
       " 'selected',\n",
       " 'proven',\n",
       " '100mg',\n",
       " 'volume',\n",
       " 'haloney',\n",
       " 'foaming',\n",
       " 'you',\n",
       " 'food',\n",
       " 'either',\n",
       " 'here',\n",
       " 'shape',\n",
       " 'may',\n",
       " 'come',\n",
       " 'moderately',\n",
       " '90563',\n",
       " 'automatic',\n",
       " 'group',\n",
       " 'fbi',\n",
       " 'york',\n",
       " 'net',\n",
       " 'jqplot',\n",
       " '2011',\n",
       " 'forum',\n",
       " 'rude',\n",
       " 'microsoft',\n",
       " 'individual',\n",
       " 'province',\n",
       " 'permanantly',\n",
       " 'money',\n",
       " '430',\n",
       " 'finder',\n",
       " 'yeah']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建词汇表\n",
    "vocabList = createVocabList(docList)\n",
    "trainingSet = list(range(50))\n",
    "testSet = []\n",
    "\n",
    "# 随机取 10 个邮件用来测试\n",
    "for _ in range(10):\n",
    "    # random.uniform(x, y) 随机生成一个范围为 x - y 的实数\n",
    "    randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "    testSet.append(trainingSet[randIndex])\n",
    "    del(trainingSet[randIndex])\n",
    "\n",
    "trainMat = []\n",
    "trainClasses = []\n",
    "for docIndex in trainingSet:\n",
    "    trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "    trainClasses.append(classList[docIndex])\n",
    "p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.475"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
